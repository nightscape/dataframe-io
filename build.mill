import $ivy.`com.lihaoyi::mill-contrib-sonatypecentral:0.12.8`
import mill.contrib.sonatypecentral.SonatypeCentralPublishModule

import coursier.maven.MavenRepository
import mill._, scalalib._, scalalib.publish._, scalafmt._
import mill.define.Cross.Resolver
import mill.scalalib.Assembly._
import $ivy.`com.lihaoyi::mill-contrib-buildinfo:`
import mill.contrib.buildinfo.BuildInfo
import $ivy.`io.chris-kipp::mill-ci-release::0.3.0`
import io.kipp.mill.ci.release.CiReleaseModule
import de.tobiasroeser.mill.vcs.version.VcsVersion

val url = "https://github.com/nightscape/dataframe-io"
object build extends Module {
  def publishVersion = T {
    VcsVersion.vcsState().format(untaggedSuffix = "-SNAPSHOT")
  }
}
trait DfioModule
    extends CrossScalaModule
    with Cross.Module2[String, String]
    with SbtModule
    with SonatypeCentralPublishModule
    with CiReleaseModule
    with ScalafmtModule {
  val sparkVersion = crossValue2
  val Array(sparkMajor, sparkMinor, sparkPatch) = sparkVersion.split("\\.")
  val sparkBinaryVersion = s"$sparkMajor.$sparkMinor"
  override def artifactNameParts: T[Seq[String]] =
    Seq(super.artifactNameParts().head, "spark", sparkBinaryVersion)

  def compileIvyDeps =
    Agg(ivy"org.apache.spark::spark-core:$sparkVersion", ivy"org.apache.spark::spark-sql:$sparkVersion")
  def pomSettings = PomSettings(
    description = "A Spark library for reading and writing from/to various data sources",
    organization = "dev.mauch.spark.dfio",
    url = url,
    licenses = Seq(License.MIT),
    versionControl = VersionControl(browsableRepository = Some(url), connection = Some(s"scm:git:$url.git")),
    developers = Seq(Developer("nightscape", "Martin Mauch", "https://github.com/nightscape"))
  )
  override implicit def crossSbtModuleResolver: Resolver[CrossModuleBase] = new Resolver[CrossModuleBase] {
    def resolve[V <: CrossModuleBase](c: Cross[V]): V = c.valuesToModules(List(crossValue, crossValue2))
  }
}

trait HwcModule extends SbtModule {
  def repositoriesTask = T.task {
    super.repositoriesTask() ++ Seq(MavenRepository("https://repository.cloudera.com/artifactory/libs-release-local/"))
  }
}

trait ConfluentRepo extends ScalaModule {
  def repositoriesTask = T.task {
    super.repositoriesTask() ++ Seq(MavenRepository("https://packages.confluent.io/maven/"))
  }
}
val scala213 = "2.13.16"
val scala212 = "2.12.20"
val spark30 = List("3.0.3")
val spark31 = List("3.1.3")
val spark32 = List("3.2.4")
val spark33 = List("3.3.4")
val spark34 = List("3.4.1", "3.4.4")
val spark35 = List("3.5.5")
val sparkVersions = spark30 ++ spark31 ++ spark32 ++ spark33 ++ spark34 ++ spark35
val crossMatrix212 = sparkVersions.map(spark => (scala212, spark))
val crossMatrix213 = sparkVersions.filter(_ >= "3.2").map(spark => (scala213, spark))
val crossMatrix = crossMatrix212 ++ crossMatrix213

object core extends Cross[DfioModule](crossMatrix)

trait SerdeModule extends DfioModule with ConfluentRepo {
  def moduleDeps = Seq(core())
  def ivyDeps = Agg(
    ivy"org.scala-lang.modules::scala-xml:2.3.0",
    ivy"com.fasterxml.jackson.module::jackson-module-scala:2.18.3",
    ivy"za.co.absa::abris:6.4.1"
  )
}
object serde extends Cross[SerdeModule](crossMatrix)

trait HiveModule extends DfioModule with HwcModule {
  def moduleDeps = Seq(core())
  def compileIvyDeps = Agg(
    ivy"com.hortonworks.hive::hive-warehouse-connector-spark3:1.0.0.7.2.17.0-334"
      .exclude("net.minidev" -> "json-smart")
      .exclude("net.shibboleth.tool" -> "xmlsectool")
      .exclude("org.cloudera.logredactor" -> "logredactor")
      .excludeOrg("androidx.annotation")
  )
}
object hive extends Cross[HiveModule](crossMatrix212)

trait DeltaModule extends DfioModule {
  def moduleDeps = Seq(core())
  val deltaVersion = sparkVersion match {
    case v if v.startsWith("3.0") => "0.8.0"
    case v if v.startsWith("3.1") => "1.0.0"
    case v if v.startsWith("3.2") => "2.0.0"
    case v if v.startsWith("3.3") => "2.3.0"
    case v if v.startsWith("3.4") => "2.4.0"
    case _ => "3.3.0"
  }
  val artifact = deltaVersion.split("\\.").head match {
    case "0" | "1" | "2" => "delta-core"
    case _ => "delta-spark"
  }
  def ivyDeps = Agg(ivy"io.delta::$artifact:$deltaVersion")
}
object delta extends Cross[DeltaModule](crossMatrix)

trait ExcelModule extends DfioModule {
  def moduleDeps = Seq(core())
  def ivyDeps = Agg(ivy"dev.mauch::spark-excel:${sparkVersion}_0.30.2")
}
object excel extends Cross[ExcelModule](crossMatrix)

trait KafkaModule extends DfioModule with ConfluentRepo {
  def moduleDeps = Seq(core(), serde())
  def ivyDeps = Agg(ivy"org.apache.spark::spark-sql-kafka-0-10:$sparkVersion")
}
object kafka extends Cross[KafkaModule](crossMatrix)

trait SolrModule extends DfioModule {
  def moduleDeps = Seq(core())
  def ivyDeps = Agg(ivy"org.apache.solr:solr-solrj:9.8.1", ivy"commons-codec:commons-codec:1.18.0")
}
object solr extends Cross[SolrModule](crossMatrix)

trait UriParserModule extends DfioModule with HwcModule with ConfluentRepo {
  def moduleDeps = Seq(core(), serde())
  def ivyDeps = Agg(ivy"org.scala-lang.modules::scala-collection-compat:2.13.0")
  def scalacOptions = Seq("-Xexperimental")
}
object `uri-parser` extends Cross[UriParserModule](crossMatrix)

trait EtlModule extends DfioModule with HwcModule with ConfluentRepo with BuildInfo { etlModule =>
  def mainClass = Some("dev.mauch.spark.dfio.ETL")
  // def finalMainClassOpt = Left[String, String]("none")
  def moduleDeps = Seq(core(), serde(), `uri-parser`())
  def ivyDeps = Agg(ivy"org.apache.spark::spark-sql:$sparkVersion", ivy"com.lihaoyi::mainargs:0.6.3").map(
    _.excludeOrg("androidx.annotation")
  )
  def assemblyRules = super.assemblyRules ++ Seq(
    Rule.AppendPattern("META-INF/services/.*"),
    Rule.Append("application.conf"), // all application.conf files will be concatenated into single file
    Rule.AppendPattern(".*\\.conf"), // all *.conf files will be concatenated into single file
    Rule.ExcludePattern(".*.temp"), // all *.temp files will be excluded from a final jar
    Rule.ExcludePattern("scala/.*"),
    Rule.ExcludePattern("org\\.apache\\.spark/.*")
  )
  def buildInfoPackageName = "dev.mauch.spark.dfio"
  def buildInfoMembers =
    Seq(
      BuildInfo.Value("version", publishVersion()),
      BuildInfo.Value("formattedShaVersion", "TODO"),
      BuildInfo.Value("scalaVersion", scalaVersion()),
      BuildInfo.Value("sparkVersion", sparkVersion),
      BuildInfo.Value("readme", os.read(millSourcePath / os.up / "README.md"))
    )
  def resources = super.resources() ++ Seq(PathRef(millSourcePath / os.up / "README.md"))

    object test extends SbtTests with TestModule.ZioTest {
      def moduleDeps = super.moduleDeps ++ etlModule.moduleDeps ++ Seq(kafka(), delta())
      def ivyDeps = Agg(
        ivy"dev.zio::zio-test:2.1.14",
        ivy"dev.zio::zio-test-sbt:2.1.14",
        ivy"com.dimafeng::testcontainers-scala-core:0.41.8",
        ivy"org.testcontainers:testcontainers:1.20.4",
        ivy"com.dimafeng::testcontainers-scala-kafka:0.41.8",
        ivy"dev.zio::zio-kafka:2.6.0",
        ivy"org.apache.kafka:kafka-clients:3.6.2",
        ivy"com.github.sideeffffect::zio-testcontainers:0.6.0",
        ivy"org.apache.spark::spark-sql:$sparkVersion",
      )
    }
}
object etl extends Cross[EtlModule](crossMatrix)
